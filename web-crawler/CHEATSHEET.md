# Web Crawler - Quick Cheatsheet

> â±ï¸ **5-minute revision before interview**

---

## ğŸ“‹ Requirements Summary

### Functional Requirements
| # | Requirement |
|---|-------------|
| 1 | Crawl web starting from seed URLs |
| 2 | Extract text data and store for processing |

### Non-Functional Requirements
| # | Requirement | Target |
|---|-------------|--------|
| 1 | Fault Tolerance | Resume without losing progress |
| 2 | Politeness | Respect robots.txt, 1 req/sec/domain |
| 3 | Efficiency | Complete in 5 days |
| 4 | Scale | 10 billion pages |

---

## ğŸ—ï¸ High-Level Architecture

```mermaid
flowchart TB
    subgraph Stage1[Stage 1: Fetching]
        FQ[(Fetch Queue)]
        Fetchers[URL Fetchers]
        HTML[(S3 HTML)]
    end

    subgraph Stage2[Stage 2: Parsing]
        PQ[(Parse Queue)]
        Parsers[Parser Workers]
        Text[(S3 Text)]
    end

    subgraph Control[Control]
        Meta[(Metadata DB)]
        Redis[(Redis Rate Limit)]
    end

    FQ --> Fetchers --> HTML --> PQ --> Parsers --> Text
    Parsers --> FQ
    Fetchers <--> Redis
    Fetchers & Parsers <--> Meta
```

---

## ğŸ” Deep Dives

### 1. Fault Tolerance

| Approach | Description | Verdict |
|----------|-------------|---------|
| âŒ **Monolithic Crawler** | One service does fetch + parse + extract | Bad - Single failure loses all progress |
| âœ… **Multi-Stage Pipeline** | Separate fetch and parse stages | Good - Isolate failures, retry independently |

```
âœ… GOOD:
   Fetcher fails â†’ URL stays in queue â†’ Another fetcher picks up
   Parser fails â†’ HTML still in S3 â†’ Just re-parse

âŒ BAD:
   Monolithic crawler fails mid-parse â†’ Lost fetched HTML â†’ Refetch everything
```

**SQS Retry Pattern:**
```
Message â†’ Fetch attempt â†’ Failure â†’ Visibility timeout â†’ Retry
                                                â†“
                                    After 3 retries â†’ Dead Letter Queue
```

### 2. Politeness & robots.txt

| Approach | Description | Verdict |
|----------|-------------|---------|
| âŒ **Ignore robots.txt** | Crawl everything | Bad - Get blocked, unethical |
| âŒ **Per-crawler rate limit** | Each crawler limits itself | Bad - N crawlers = NÃ— requests |
| âœ… **Global rate limit** | Redis-based domain limiting | Good - True 1 req/sec enforcement |

```
âœ… GOOD:
   1. Check robots.txt (cached) â†’ Allowed?
   2. Check Redis rate limit â†’ Within 1 req/sec?
   3. If both OK â†’ Crawl
   4. If rate limited â†’ Re-queue with delay + jitter

âŒ BAD:
   Multiple crawlers â†’ All hit example.com simultaneously â†’ Server overload â†’ Blocked!
```

**robots.txt Example:**
```
User-agent: *
Disallow: /private/
Crawl-delay: 10   # Wait 10 seconds between requests
```

### 3. Scaling to 10B Pages in 5 Days

| Approach | Description | Verdict |
|----------|-------------|---------|
| âŒ **Single machine** | One crawler does everything | Bad - Takes 15+ days |
| âœ… **Parallel fetchers** | 4 machines Ã— 7.5k pages/s | Good - ~30k pages/s = 4 days |

**Math:**
```
Required rate: 10B pages Ã· 5 days Ã· 86400 sec = 23,148 pages/sec

Per machine (30% efficiency): 7,500 pages/sec
Machines needed: 23,148 Ã· 7,500 = 3.1 â†’ 4 machines
```

### 4. DNS Optimization

| Approach | Description | Verdict |
|----------|-------------|---------|
| âŒ **DNS per request** | Lookup every time | Bad - Hit rate limits, slow |
| âœ… **DNS caching** | Cache lookups locally | Good - Reuse for same domain |
| âœ… **Multiple providers** | Cloudflare + Google + Route53 | Good - Avoid single provider limits |

```
âœ… GOOD: Cache DNS + Multiple providers (round-robin)
âŒ BAD:  Every URL â†’ DNS lookup â†’ Rate limited â†’ Blocked
```

### 5. Avoiding Duplicate Work

| Approach | Description | Verdict |
|----------|-------------|---------|
| âŒ **No dedup** | Crawl everything | Bad - Waste resources |
| âœ… **URL check in DB** | Check before queueing | Good - Simple, effective |
| âœ… **Content hash** | Hash page content | Good - Catches same content, different URLs |
| âš ï¸ **Bloom filter** | Probabilistic dedup | OK - Memory efficient, false positives |

```
âœ… GOOD:
   Before queue: Check Metadata DB â†’ URL exists? Skip!
   After crawl: Hash content â†’ Same hash? Duplicate!

âŒ BAD: Queue all URLs, dedupe later â†’ Wasted fetches
```

### 6. Crawler Traps

| Approach | Description | Verdict |
|----------|-------------|---------|
| âŒ **No depth limit** | Follow all links forever | Bad - Infinite loops |
| âœ… **Max depth** | Stop at depth 15-20 | Good - Prevents traps |

```
âœ… GOOD: Track depth per URL â†’ Stop if depth > 15
âŒ BAD:  /page/1 â†’ /page/2 â†’ /page/3 â†’ ... â†’ /page/999999 (infinite!)
```

---

## ğŸ“Š Key Numbers

| Metric | Value |
|--------|-------|
| Total pages | 10 billion |
| Average page | 2 MB |
| Time limit | 5 days |
| Required rate | ~23,000 pages/sec |
| Machines needed | 4 |
| Rate limit | 1 req/sec/domain |
| Max crawl depth | 15-20 |
| Raw HTML storage | ~20 PB |

---

## ğŸ’¬ Interview Phrases

1. *"Pipeline stages isolate failures - HTML in S3 survives parser crashes"*
2. *"Global Redis rate limiting ensures true 1 req/sec per domain across all crawlers"*
3. *"DNS caching + multiple providers avoids DNS bottleneck"*
4. *"Max depth prevents crawler traps from infinite loops"*
5. *"Content hashing catches duplicate content across different URLs"*

---

## âš ï¸ Pitfalls to Avoid

1. âŒ Monolithic crawler (all-or-nothing failure)
2. âŒ Ignoring robots.txt (get blocked, unethical)
3. âŒ Per-crawler rate limiting (ineffective at scale)
4. âŒ DNS lookup per request (bottleneck)
5. âŒ No crawler trap protection (infinite loops)
6. âŒ No content deduplication (wasted work)

---

## ğŸ—ï¸ Technology Stack

| Component | Technology | Why |
|-----------|------------|-----|
| Queue | SQS | Built-in retries, DLQ, visibility timeout |
| Blob Storage | S3 | Cheap, durable, scalable |
| Metadata DB | DynamoDB | Key-value, scalable |
| Rate Limiting | Redis | Fast, distributed counters |
| DNS | Multiple providers | Avoid single point rate limits |
