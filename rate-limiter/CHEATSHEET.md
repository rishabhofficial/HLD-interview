# Rate Limiter - Quick Cheatsheet

> ‚è±Ô∏è **5-minute revision before interview**

---

## üìã Requirements Summary

### Functional Requirements
| # | Requirement |
|---|-------------|
| 1 | Identify clients by user ID, IP address, or API key |
| 2 | Limit requests based on configurable rules |
| 3 | Reject with HTTP 429 + helpful headers |

### Non-Functional Requirements
| # | Requirement | Target |
|---|-------------|--------|
| 1 | Low Latency | < 10ms overhead |
| 2 | High Availability | 99.99% uptime |
| 3 | Scalability | 1M requests/second |
| 4 | Consistency | Eventual (acceptable) |

---

## üèóÔ∏è High-Level Architecture

```mermaid
flowchart TB
    subgraph Clients
        U[Users]
    end

    subgraph Gateway[API Gateway Layer]
        LB[Load Balancer]
        GW[Gateway + Rate Limiter]
    end

    subgraph Redis[Redis Cluster]
        S1[(Shard 1)]
        S2[(Shard 2)]
        SN[(Shard N)]
    end

    subgraph Backend
        API[API Servers]
    end

    U --> LB --> GW
    GW <-->|Token Bucket State| S1 & S2 & SN
    GW -->|If allowed| API
```

---

## üîç Deep Dives

### 1. Rate Limiting Algorithms

| Algorithm | Description | Verdict |
|-----------|-------------|---------|
| ‚ùå **Fixed Window** | Count per time bucket | Bad - 2x burst at boundary |
| ‚ö†Ô∏è **Sliding Window Log** | Track all timestamps | OK - Perfect accuracy, O(n) memory |
| ‚ö†Ô∏è **Sliding Window Counter** | Weighted previous + current | OK - Approximation |
| ‚úÖ **Token Bucket** | Bucket with refill rate | Good - Handles bursts, O(1) memory |

```
‚úÖ GOOD: Token Bucket - natural burst handling, simple, O(1)
‚ùå BAD:  Fixed Window - user sends 100 at 12:00:59, 100 at 12:01:00 = 200 in 2s!
```

### 2. Rate Limiter Placement

| Placement | Description | Verdict |
|-----------|-------------|---------|
| ‚ùå **In-Process** | Inside each service | Bad - Can't share state across instances |
| ‚úÖ **API Gateway** | Centralized at gateway | Good - Shared state, natural fit |
| ‚ö†Ô∏è **Sidecar** | Service mesh pattern | OK - Added complexity |

```
‚úÖ GOOD: All gateways ‚Üí Single Redis Cluster ‚Üí Consistent limits
‚ùå BAD:  Each service has own counter ‚Üí Different views of same user
```

### 3. Redis Atomicity (Race Condition Fix)

| Approach | Description | Verdict |
|----------|-------------|---------|
| ‚ùå **Separate Read/Write** | HMGET then HSET | Bad - Race condition! |
| ‚úÖ **Lua Script** | Atomic read-modify-write | Good - No race |

```
‚ùå BAD:
   Gateway A reads: 1 token    |    Gateway B reads: 1 token
   Both allow request          |    Both allow request
   Result: 2 requests with 1 token!

‚úÖ GOOD: Lua script does everything atomically in Redis
```

### 4. Failure Mode

| Mode | Description | Verdict |
|------|-------------|---------|
| ‚ö†Ô∏è **Fail-Open** | Allow all when Redis down | OK for non-critical |
| ‚úÖ **Fail-Closed** | Reject all when Redis down | Good for critical systems |

```
‚úÖ GOOD (Critical): Redis down ‚Üí Reject all ‚Üí Protect backend from cascade
‚ùå BAD (Critical):  Redis down ‚Üí Allow all ‚Üí DDoS overwhelms backend
```

### 5. Scaling

| Component | Strategy |
|-----------|----------|
| Redis | Shard by client_id using consistent hashing |
| Gateways | Horizontal scaling |
| Connections | Connection pooling (-50ms latency) |
| Geographic | Deploy Redis per region |

```
‚úÖ GOOD: 10 Redis shards √ó 100k ops/s = 1M ops/s capacity
‚ùå BAD:  Single Redis ‚Üí 100k ops/s max ‚Üí Bottleneck
```

### 6. Hot Keys

| Scenario | Solution |
|----------|----------|
| Legitimate high-volume | Client-side rate limiting, premium tiers |
| Abusive traffic | Auto-block after N hits, blocklist |

---

## üìä Key Numbers

| Metric | Value |
|--------|-------|
| Target RPS | 1,000,000 |
| Redis per instance | 100-200k ops/s |
| Shards needed | 10-15 |
| Latency target | < 10ms |
| Memory per bucket | ~50 bytes |

---

## üí¨ Interview Phrases

1. *"Token Bucket handles bursty traffic naturally"*
2. *"Lua scripts make read-modify-write atomic in Redis"*
3. *"We shard by client ID using consistent hashing"*
4. *"Fail-closed protects backend during outages"*
5. *"Connection pooling eliminates TCP handshake overhead"*

---

## ‚ö†Ô∏è Pitfalls to Avoid

1. ‚ùå Separate read/write in Redis (race condition!)
2. ‚ùå Fixed window for strict limits
3. ‚ùå Single Redis instance at scale
4. ‚ùå Fail-open during DDoS
5. ‚ùå New TCP connection per request
